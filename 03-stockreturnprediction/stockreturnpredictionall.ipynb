{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ec9a8d",
   "metadata": {},
   "source": [
    "# Stock Price Prediction\n",
    "\n",
    "**Author**: Arnav Sharma - arnav.sharma2264@gmail.com - github.com/ArnavSharma2\n",
    "**Date**: 2025-08-30 \n",
    "\n",
    "**Description**: This program implements all the stock csvs in the Dataset -> train -> stocks folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f89ed",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "Install and import necessary libraries, set random seeds for reproducibility, and configure project settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8a54c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aps/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment to install)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "# from flaml import AutoML\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import shap\n",
    "# from skopt import BayesSearchCV\n",
    "from joblib import dump, load \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "#gender, PaymentMethod, MonthlyCharges, Dependents\n",
    "PATH = os.getcwd()\n",
    "from pathlib import Path\n",
    "\n",
    "folder_to_scan = Path(\"Dataset/stock-price-prediction-challenge/train/stocks/\")\n",
    "\n",
    "DATA_LIST = []\n",
    "TICKER_LIST = os.listdir(folder_to_scan)\n",
    "for item in folder_to_scan.iterdir():\n",
    "    DATA_LIST.append(item)\n",
    "    \n",
    "DATA_PATH = os.path.join(PATH, DATA_LIST[0])\n",
    "\n",
    "# Define project-specific variables\n",
    "# DATA_PATH = \"/Users/aps/Desktop/ML-DL-Projects/01-housepriceprediction/Dataset/HousePrices.csv\"  # Update with your dataset path\n",
    "TARGET_COLUMN = \"Returns\"  # Update with your target column name\n",
    "# DROP_COLUMNS = ['yr_renovated', 'yr_built', 'condition', 'sqft_lot', 'country', 'waterfront', 'floors', 'date','street'] # list of columns to drop\n",
    "DROP_COLUMNS = ['Adjusted', 'Ticker', 'Date']\n",
    "HANDLE_OUTLIERS = ['Returns', 'Volume']\n",
    "TASK_TYPE = \"regression\"  # Options: \"classification\" or \"regression\"\n",
    "MODEL_SAVE_PATH = \"model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5582c9",
   "metadata": {},
   "source": [
    "## 2. Complete Data Loading, Data Preprocessing, Model creation, and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04157270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "                     Ticker      RMSE  R2 Score\n",
      "Linear Regression  CSCO.csv  0.175485  0.953263\n",
      "Random Forest      CSCO.csv  0.459072  0.680156\n",
      "XGBoost            CSCO.csv  0.385127  0.774896\n",
      "Deep Learning      CSCO.csv  0.122671  0.977162\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "                   Ticker      RMSE  R2 Score\n",
      "Linear Regression  BA.csv  0.279639  0.886033\n",
      "Random Forest      BA.csv  0.572585  0.522182\n",
      "XGBoost            BA.csv  0.391251  0.776903\n",
      "Deep Learning      BA.csv  0.139287  0.971725\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "                    Ticker      RMSE  R2 Score\n",
      "Linear Regression  TDW.csv  0.652825  0.277242\n",
      "Random Forest      TDW.csv  0.582699  0.424179\n",
      "XGBoost            TDW.csv  0.513886  0.552151\n",
      "Deep Learning      TDW.csv  0.663446  0.253534\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "                     Ticker      RMSE  R2 Score\n",
      "Linear Regression  BOOT.csv  0.565132  0.551809\n",
      "Random Forest      BOOT.csv  0.565629   0.55102\n",
      "XGBoost            BOOT.csv  0.516124  0.626172\n",
      "Deep Learning      BOOT.csv  0.454083  0.710643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 230\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m models, X_train, X_test, y_train, y_test\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m models, X_train, X_test, y_train, y_test = \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# model eval\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmodel_weights\u001b[39m(mod):\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# 3. Access and print the weights\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 199\u001b[39m, in \u001b[36mtrain_models\u001b[39m\u001b[34m(X, y)\u001b[39m\n\u001b[32m    196\u001b[39m     dl_model.compile(optimizer=Adam(learning_rate=\u001b[32m0.001\u001b[39m), loss=\u001b[33m'\u001b[39m\u001b[33mmse\u001b[39m\u001b[33m'\u001b[39m, metrics=[\u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Train Deep Learning model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mdl_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m models[\u001b[33m'\u001b[39m\u001b[33mDeep Learning\u001b[39m\u001b[33m'\u001b[39m] = dl_model\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# print(\"Deep Learning model trained successfully!\")\u001b[39;00m\n\u001b[32m    202\u001b[39m \n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# X_train_seq, y_train_seq = create_sequences(X_train, y_train)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m \u001b[38;5;66;03m#automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m#models['AutoML'] = automl_model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:401\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_eval_epoch_iterator\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m._eval_epoch_iterator = TFEpochIterator(\n\u001b[32m    392\u001b[39m         x=val_x,\n\u001b[32m    393\u001b[39m         y=val_y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m         shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    400\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m val_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m val_logs = {\n\u001b[32m    412\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mval_\u001b[39m\u001b[33m\"\u001b[39m + name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs.items()\n\u001b[32m    413\u001b[39m }\n\u001b[32m    414\u001b[39m epoch_logs.update(val_logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:487\u001b[39m, in \u001b[36mTensorFlowTrainer.evaluate\u001b[39m\u001b[34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28mself\u001b[39m.reset_metrics()\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_test_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbegin_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:742\u001b[39m, in \u001b[36mTFEpochIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/keras/src/trainers/epoch_iterator.py:125\u001b[39m, in \u001b[36mEpochIterator._enumerate_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[32m    120\u001b[39m             step,\n\u001b[32m    121\u001b[39m             step + \u001b[38;5;28mself\u001b[39m.steps_per_execution - \u001b[32m1\u001b[39m,\n\u001b[32m    122\u001b[39m             \u001b[38;5;28mself\u001b[39m._current_iterator,\n\u001b[32m    123\u001b[39m         )\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._steps_seen >= \u001b[38;5;28mself\u001b[39m._num_batches:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m         \u001b[38;5;28mself\u001b[39m._current_iterator = \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m         \u001b[38;5;28mself\u001b[39m._steps_seen = \u001b[32m0\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001b[39m, in \u001b[36mDatasetV2.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context.executing_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops.inside_function():\n\u001b[32m    500\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(\u001b[38;5;28mself\u001b[39m._variant_tensor):\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33miteration in eager mode or within tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/tensorflow/python/data/ops/iterator_ops.py:709\u001b[39m, in \u001b[36mOwnedIterator.__init__\u001b[39m\u001b[34m(self, dataset, components, element_spec)\u001b[39m\n\u001b[32m    705\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    707\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    708\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot be specified.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28mself\u001b[39m._get_next_call_count = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/tensorflow/python/data/ops/iterator_ops.py:748\u001b[39m, in \u001b[36mOwnedIterator._create_iterator\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    745\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype.args[\u001b[32m0\u001b[39m].args[\u001b[32m0\u001b[39m].args) == \u001b[38;5;28mlen\u001b[39m(\n\u001b[32m    746\u001b[39m       \u001b[38;5;28mself\u001b[39m._flat_output_types)\n\u001b[32m    747\u001b[39m   \u001b[38;5;28mself\u001b[39m._iterator_resource.op.experimental_set_type(fulltype)\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001b[39m, in \u001b[36mmake_iterator\u001b[39m\u001b[34m(dataset, iterator, name)\u001b[39m\n\u001b[32m   3476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   3477\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3478\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3479\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMakeIterator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3480\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3481\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "filec = 0\n",
    "for file in DATA_LIST:\n",
    "    DATA_PATH = os.path.join(PATH, file)\n",
    "    # print(f\"\\n\\n{TICKER_LIST[filec]}\")\n",
    "    filec+=1\n",
    "    def load_data(file_path):\n",
    "        \"\"\"\n",
    "        Load dataset from a given file path.\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (str): Path to the dataset (CSV, Excel, etc.)\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Loaded dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file_path.endswith('.csv'):\n",
    "                data = pd.read_csv(file_path)\n",
    "            elif file_path.endswith('.xlsx'):\n",
    "                data = pd.read_excel(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format\")\n",
    "            \n",
    "            # print(\"Data loaded successfully!\")\n",
    "            # print(f\"Shape: {data.shape}\")\n",
    "            # print(\"\\nFirst 5 rows:\")\n",
    "            # print(data.head())\n",
    "            # print(\"\\nData Info:\")\n",
    "            # print(data.info())\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    # Load the dataset\n",
    "    df = load_data(DATA_PATH)\n",
    "    if df is None:\n",
    "        raise SystemExit(\"Data loading failed. Exiting.\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def transform_with_ohe(ohe_df, colname):\n",
    "        ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
    "        ohe1 = ohe.fit_transform(ohe_df[[colname]])\n",
    "        ohe2 = pd.DataFrame(ohe1, columns=ohe.get_feature_names_out([colname]), index=ohe_df.index)\n",
    "        transformed = pd.concat([ohe_df.drop(colname, axis=1), ohe2], axis=1)\n",
    "        transformed.head()\n",
    "        return transformed\n",
    "\n",
    "    def transform_with_le(df, col):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        return df\n",
    "\n",
    "    def remove_any_outliers(outlier_df, colname):\n",
    "        # This line of code keeps only the middle 98% of price values and removes the most extreme 2% (lowest 1% + highest 1%) from your dataset.\n",
    "        # Keeps only the rows where price is greater than the 1st percentile and less than the 99th percentile.\n",
    "        # In other words, it removes the extreme 1% lowest and 1% highest values.\n",
    "        high = 0.99\n",
    "        low = 1-high\n",
    "        \n",
    "        removed_outliers = outlier_df[(outlier_df[colname] > outlier_df[colname].quantile(low)) & \n",
    "                            (outlier_df[colname] < outlier_df[colname].quantile(high))]\n",
    "        removed_outliers.columns\n",
    "        # # Plot the Before and After graph\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        # # Before\n",
    "        # sns.boxplot(x=outlier_df[colname], ax=axes[0])\n",
    "        # axes[0].set_title(\"Before Removing Outliers\")\n",
    "        # # After\n",
    "        # sns.boxplot(x=removed_outliers[colname], ax=axes[1])\n",
    "        # axes[1].set_title(\"After Removing Outliers\")\n",
    "        # plt.show()\n",
    "        return removed_outliers\n",
    "\n",
    "\n",
    "    def preprocess_data(df, target_column):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset (handle missing values, encode categorical variables, etc.).\n",
    "        \n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): Input dataset\n",
    "        target_column (str): Name of the target column\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Preprocessed dataset\n",
    "        \"\"\"\n",
    "        # # Separate features and target\n",
    "        # X = df.drop(columns=[target_column])\n",
    "        # y = df[target_column]\n",
    "        # removing unimportant features\n",
    "        processed = df.drop(DROP_COLUMNS, axis=1)\n",
    "        labelencoding = ['']\n",
    "        onehotencoding = ['']\n",
    "        # Handle missing values\n",
    "        numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "        categorical_cols = processed.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        processed['Returns'] = (processed['Close']-processed['Open'])/processed['Open']\n",
    "\n",
    "        \n",
    "        # Impute numeric columns with median\n",
    "        for col in numeric_cols:\n",
    "            processed[col].fillna(processed[col].median(), inplace=True)\n",
    "        \n",
    "        # Impute categorical columns with mode, Grab the final DF before OHE so we can do SHAP\n",
    "        # for col in categorical_cols:\n",
    "        #     SHAP_DF = processed[col].fillna(processed[col].mode()[0], inplace=True)\n",
    "\n",
    "        # Encode categorical variables\n",
    "        # le = LabelEncoder()\n",
    "        # for col in categorical_cols:\n",
    "        #     X[col] = le.fit_transform(X[col])\n",
    "\n",
    "        # for col in labelencoding:\n",
    "        #     processed = transform_with_le(processed, col)\n",
    "\n",
    "        # Scale the numerical columns\n",
    "        scaler = StandardScaler()\n",
    "        numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "        processed[numeric_cols] = scaler.fit_transform(processed[numeric_cols])\n",
    "        \n",
    "        # Handle any outlier needs\n",
    "        for col in HANDLE_OUTLIERS:\n",
    "            processed = remove_any_outliers(processed,col)\n",
    "        # split into X and Y DF\n",
    "        X = processed.drop(target_column,axis=1)\n",
    "        y = processed[target_column]\n",
    "\n",
    "        # print(\"Data preprocessing completed!\")\n",
    "        #return X, y, scaler\n",
    "        return X, y\n",
    "\n",
    "    # Preprocess data\n",
    "    # X, y, scaler = preprocess_data(df, TARGET_COLUMN)\n",
    "    X, y = preprocess_data(df, TARGET_COLUMN)\n",
    "\n",
    "    # def create_sequences(X, y, time_steps=60):\n",
    "    #     \"\"\"\n",
    "    #     Reshapes 2D data into 3D sequences for LSTM models.\n",
    "    #     \"\"\"\n",
    "    #     X_seq, y_seq = [], []\n",
    "    #     for i in range(len(X) - time_steps):\n",
    "    #         # Take a sequence of 'time_steps' length\n",
    "    #         X_seq.append(X[i:(i + time_steps)])\n",
    "    #         # The corresponding label is the value that comes immediately after the sequence\n",
    "    #         y_seq.append(y[i + time_steps])\n",
    "    #     return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "    #- ---------------------------------------------------------------------------------------------------------------------\n",
    "    # train models\n",
    "    def train_models(X, y):\n",
    "        \"\"\"\n",
    "        Train multiple machine learning models, AutoML, and a deep learning model.\n",
    "        \n",
    "        Parameters:\n",
    "        X (pandas.DataFrame): Features\n",
    "        y (numpy.array): Target\n",
    "        \n",
    "        Returns:\n",
    "        dict: Trained models (including AutoML and Deep Learning)\n",
    "        \"\"\"\n",
    "        # Split data into train and test sets\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        \n",
    "        # Initialize models based on task type\n",
    "        models = {}\n",
    "        if TASK_TYPE == \"classification\":\n",
    "            models['Logistic Regression'] = LogisticRegression(random_state=42)\n",
    "            models['Random Forest'] = RandomForestClassifier(random_state=42)\n",
    "            models['XGBoost'] = XGBClassifier(random_state=42)\n",
    "        else:\n",
    "            models['Linear Regression'] = LinearRegression()\n",
    "            models['Random Forest'] = RandomForestRegressor(random_state=42)\n",
    "            models['XGBoost'] = XGBRegressor(random_state=42)\n",
    "\n",
    "        # Train manual models\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            # print(f\"{name} trained successfully!\")\n",
    "        \n",
    "            # Deep Learning Model (Simple Neural Network)\n",
    "        X_copy = X_train\n",
    "        y_copy = y_train\n",
    "        dl_model = Sequential()\n",
    "        dl_model.add(Dense(64, activation='relu', input_shape=(X_copy.shape[1],)))\n",
    "        dl_model.add(Dropout(0.2))\n",
    "        dl_model.add(Dense(32, activation='relu'))\n",
    "        dl_model.add(Dropout(0.2))\n",
    "        if TASK_TYPE == \"classification\":\n",
    "            dl_model.add(Dense(1, activation='sigmoid'))\n",
    "            dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        else:\n",
    "            dl_model.add(Dense(1))\n",
    "            dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Train Deep Learning model\n",
    "        dl_model.fit(X_copy, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
    "        models['Deep Learning'] = dl_model\n",
    "        # print(\"Deep Learning model trained successfully!\")\n",
    "\n",
    "        # X_train_seq, y_train_seq = create_sequences(X_train, y_train)\n",
    "        # # LSTM Model\n",
    "        # time_steps = 60\n",
    "        # lstm_model = Sequential()\n",
    "        # lstm_model.add(LSTM(50, activation='relu', input_shape=(time_steps, X_train_seq.shape[1]), return_sequences=False))\n",
    "        # lstm_model.add(Dropout(0.2))\n",
    "        # lstm_model.add(Dense(25, activation='relu'))\n",
    "        # lstm_model.add(Dropout(0.2))\n",
    "        # if TASK_TYPE == \"classification\":\n",
    "        #     lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "        #     lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        # else:\n",
    "        #     lstm_model.add(Dense(1))\n",
    "        #     lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # # Train LSTM model\n",
    "        # lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
    "        # models['LSTM'] = lstm_model\n",
    "        # print(\"LSTM model trained successfully!\")\n",
    "        \n",
    "        # Run AutoML\n",
    "        #automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)\n",
    "        #models['AutoML'] = automl_model\n",
    "\n",
    "        return models, X_train, X_test, y_train, y_test\n",
    "\n",
    "    # Train models\n",
    "    models, X_train, X_test, y_train, y_test = train_models(X, y)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # model eval\n",
    "    def model_weights(mod):\n",
    "        # 3. Access and print the weights\n",
    "        weights = mod.coef_\n",
    "        intercept = mod.intercept_\n",
    "\n",
    "        print(f\"Weights (Coefficients): {weights}\")\n",
    "        print(f\"Intercept: {intercept}\")\n",
    "\n",
    "        # 4. Optional: Create a DataFrame for better readability\n",
    "        # This pairs each weight with its corresponding feature name\n",
    "        weights_df = pd.DataFrame({'Feature': X.columns, 'Weight': weights})\n",
    "        print(\"\\nModel Weights:\")\n",
    "        print(weights_df)\n",
    "\n",
    "    def evaluate_models(models, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate trained models using appropriate metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        models (dict): Dictionary of trained models\n",
    "        X_test (pandas.DataFrame): Test features\n",
    "        y_test (numpy.array): Test target\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            if name == 'Deep Learning':\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred = (y_pred > 0.5).astype(int).flatten() if TASK_TYPE == \"classification\" else y_pred.flatten()\n",
    "            else:\n",
    "                y_pred = model.predict(X_test)\n",
    "            \n",
    "            if TASK_TYPE == \"classification\":\n",
    "                results[name] = {\n",
    "                    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                    'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "                    'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "                    'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
    "                }\n",
    "            else:\n",
    "                results[name] = {\n",
    "                    'Ticker': TICKER_LIST[filec-1],\n",
    "                    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                    'R2 Score': r2_score(y_test, y_pred)\n",
    "                }\n",
    "\n",
    "        \n",
    "        # Display results\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        # print(\"\\nModel Evaluation Results:\")\n",
    "        # print(results_df)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    # Evaluate models\n",
    "    results_df = evaluate_models(models, X_test, y_test)\n",
    "    print(results_df)\n",
    "    \n",
    "    # results_df[TICKER_LIST[filec]] = evaluate_models(models, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
