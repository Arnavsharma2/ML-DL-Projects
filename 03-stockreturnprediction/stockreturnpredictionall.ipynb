{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ec9a8d",
   "metadata": {},
   "source": [
    "# Stock Price Prediction\n",
    "\n",
    "**Author**: Arnav Sharma - arnav.sharma2264@gmail.com - github.com/ArnavSharma2\n",
    "**Date**: 2025-08-30 \n",
    "\n",
    "**Description**: This program implements all the stock csvs in the Dataset -> train -> stocks folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f89ed",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "Install and import necessary libraries, set random seeds for reproducibility, and configure project settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to install)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "# from flaml import AutoML\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import shap\n",
    "# from skopt import BayesSearchCV\n",
    "from joblib import dump, load \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "#gender, PaymentMethod, MonthlyCharges, Dependents\n",
    "PATH = os.getcwd()\n",
    "from pathlib import Path\n",
    "\n",
    "folder_to_scan = Path(\"Dataset/stock-price-prediction-challenge/train/stocks/\")\n",
    "\n",
    "DATA_LIST = []\n",
    "TICKER_LIST = os.listdir(folder_to_scan)\n",
    "for item in folder_to_scan.iterdir():\n",
    "    DATA_LIST.append(item)\n",
    "    \n",
    "DATA_PATH = os.path.join(PATH, DATA_LIST[0])\n",
    "\n",
    "# Define project-specific variables\n",
    "# DATA_PATH = \"/Users/aps/Desktop/ML-DL-Projects/01-housepriceprediction/Dataset/HousePrices.csv\"  # Update with your dataset path\n",
    "TARGET_COLUMN = \"Returns\"  # Update with your target column name\n",
    "# DROP_COLUMNS = ['yr_renovated', 'yr_built', 'condition', 'sqft_lot', 'country', 'waterfront', 'floors', 'date','street'] # list of columns to drop\n",
    "DROP_COLUMNS = ['Adjusted', 'Ticker', 'Date']\n",
    "HANDLE_OUTLIERS = ['Returns', 'Volume']\n",
    "TASK_TYPE = \"regression\"  # Options: \"classification\" or \"regression\"\n",
    "MODEL_SAVE_PATH = \"model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5582c9",
   "metadata": {},
   "source": [
    "## 2. Complete Data Loading, Data Preprocessing, Model creation, and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04157270",
   "metadata": {},
   "outputs": [],
   "source": [
    "filec = 0\n",
    "for file in DATA_LIST:\n",
    "    DATA_PATH = os.path.join(PATH, file)\n",
    "    # print(f\"\\n\\n{TICKER_LIST[filec]}\")\n",
    "    filec+=1\n",
    "    def load_data(file_path):\n",
    "        \"\"\"\n",
    "        Load dataset from a given file path.\n",
    "        \n",
    "        Parameters:\n",
    "        file_path (str): Path to the dataset (CSV, Excel, etc.)\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Loaded dataset\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if file_path.endswith('.csv'):\n",
    "                data = pd.read_csv(file_path)\n",
    "            elif file_path.endswith('.xlsx'):\n",
    "                data = pd.read_excel(file_path)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format\")\n",
    "            \n",
    "            # print(\"Data loaded successfully!\")\n",
    "            # print(f\"Shape: {data.shape}\")\n",
    "            # print(\"\\nFirst 5 rows:\")\n",
    "            # print(data.head())\n",
    "            # print(\"\\nData Info:\")\n",
    "            # print(data.info())\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return None\n",
    "    # Load the dataset\n",
    "    df = load_data(DATA_PATH)\n",
    "    if df is None:\n",
    "        raise SystemExit(\"Data loading failed. Exiting.\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    def transform_with_ohe(ohe_df, colname):\n",
    "        ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
    "        ohe1 = ohe.fit_transform(ohe_df[[colname]])\n",
    "        ohe2 = pd.DataFrame(ohe1, columns=ohe.get_feature_names_out([colname]), index=ohe_df.index)\n",
    "        transformed = pd.concat([ohe_df.drop(colname, axis=1), ohe2], axis=1)\n",
    "        transformed.head()\n",
    "        return transformed\n",
    "\n",
    "    def transform_with_le(df, col):\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        return df\n",
    "\n",
    "    def remove_any_outliers(outlier_df, colname):\n",
    "        # This line of code keeps only the middle 98% of price values and removes the most extreme 2% (lowest 1% + highest 1%) from your dataset.\n",
    "        # Keeps only the rows where price is greater than the 1st percentile and less than the 99th percentile.\n",
    "        # In other words, it removes the extreme 1% lowest and 1% highest values.\n",
    "        high = 0.99\n",
    "        low = 1-high\n",
    "        \n",
    "        removed_outliers = outlier_df[(outlier_df[colname] > outlier_df[colname].quantile(low)) & \n",
    "                            (outlier_df[colname] < outlier_df[colname].quantile(high))]\n",
    "        removed_outliers.columns\n",
    "        # # Plot the Before and After graph\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        # # Before\n",
    "        # sns.boxplot(x=outlier_df[colname], ax=axes[0])\n",
    "        # axes[0].set_title(\"Before Removing Outliers\")\n",
    "        # # After\n",
    "        # sns.boxplot(x=removed_outliers[colname], ax=axes[1])\n",
    "        # axes[1].set_title(\"After Removing Outliers\")\n",
    "        # plt.show()\n",
    "        return removed_outliers\n",
    "\n",
    "\n",
    "    def preprocess_data(df, target_column):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset (handle missing values, encode categorical variables, etc.).\n",
    "        \n",
    "        Parameters:\n",
    "        df (pandas.DataFrame): Input dataset\n",
    "        target_column (str): Name of the target column\n",
    "        \n",
    "        Returns:\n",
    "        pandas.DataFrame: Preprocessed dataset\n",
    "        \"\"\"\n",
    "        # # Separate features and target\n",
    "        # X = df.drop(columns=[target_column])\n",
    "        # y = df[target_column]\n",
    "        # removing unimportant features\n",
    "        processed = df.drop(DROP_COLUMNS, axis=1)\n",
    "        labelencoding = ['']\n",
    "        onehotencoding = ['']\n",
    "        # Handle missing values\n",
    "        numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "        categorical_cols = processed.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        processed['Returns'] = (processed['Close']-processed['Open'])/processed['Open']\n",
    "\n",
    "        \n",
    "        # Impute numeric columns with median\n",
    "        for col in numeric_cols:\n",
    "            processed[col].fillna(processed[col].median(), inplace=True)\n",
    "        \n",
    "        # Impute categorical columns with mode, Grab the final DF before OHE so we can do SHAP\n",
    "        # for col in categorical_cols:\n",
    "        #     SHAP_DF = processed[col].fillna(processed[col].mode()[0], inplace=True)\n",
    "\n",
    "        # Encode categorical variables\n",
    "        # le = LabelEncoder()\n",
    "        # for col in categorical_cols:\n",
    "        #     X[col] = le.fit_transform(X[col])\n",
    "\n",
    "        # for col in labelencoding:\n",
    "        #     processed = transform_with_le(processed, col)\n",
    "\n",
    "        # Scale the numerical columns\n",
    "        scaler = StandardScaler()\n",
    "        numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "        processed[numeric_cols] = scaler.fit_transform(processed[numeric_cols])\n",
    "        \n",
    "        # Handle any outlier needs\n",
    "        for col in HANDLE_OUTLIERS:\n",
    "            processed = remove_any_outliers(processed,col)\n",
    "        # split into X and Y DF\n",
    "        X = processed.drop(target_column,axis=1)\n",
    "        y = processed[target_column]\n",
    "\n",
    "        # print(\"Data preprocessing completed!\")\n",
    "        #return X, y, scaler\n",
    "        return X, y\n",
    "\n",
    "    # Preprocess data\n",
    "    # X, y, scaler = preprocess_data(df, TARGET_COLUMN)\n",
    "    X, y = preprocess_data(df, TARGET_COLUMN)\n",
    "\n",
    "    # def create_sequences(X, y, time_steps=60):\n",
    "    #     \"\"\"\n",
    "    #     Reshapes 2D data into 3D sequences for LSTM models.\n",
    "    #     \"\"\"\n",
    "    #     X_seq, y_seq = [], []\n",
    "    #     for i in range(len(X) - time_steps):\n",
    "    #         # Take a sequence of 'time_steps' length\n",
    "    #         X_seq.append(X[i:(i + time_steps)])\n",
    "    #         # The corresponding label is the value that comes immediately after the sequence\n",
    "    #         y_seq.append(y[i + time_steps])\n",
    "    #     return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "    #- ---------------------------------------------------------------------------------------------------------------------\n",
    "    # train models\n",
    "    def train_models(X, y):\n",
    "        \"\"\"\n",
    "        Train multiple machine learning models, AutoML, and a deep learning model.\n",
    "        \n",
    "        Parameters:\n",
    "        X (pandas.DataFrame): Features\n",
    "        y (numpy.array): Target\n",
    "        \n",
    "        Returns:\n",
    "        dict: Trained models (including AutoML and Deep Learning)\n",
    "        \"\"\"\n",
    "        # Split data into train and test sets\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        \n",
    "        # Initialize models based on task type\n",
    "        models = {}\n",
    "        if TASK_TYPE == \"classification\":\n",
    "            models['Logistic Regression'] = LogisticRegression(random_state=42)\n",
    "            models['Random Forest'] = RandomForestClassifier(random_state=42)\n",
    "            models['XGBoost'] = XGBClassifier(random_state=42)\n",
    "        else:\n",
    "            models['Linear Regression'] = LinearRegression()\n",
    "            models['Random Forest'] = RandomForestRegressor(random_state=42)\n",
    "            models['XGBoost'] = XGBRegressor(random_state=42)\n",
    "\n",
    "        # Train manual models\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            # print(f\"{name} trained successfully!\")\n",
    "        \n",
    "            # Deep Learning Model (Simple Neural Network)\n",
    "        X_copy = X_train\n",
    "        y_copy = y_train\n",
    "        dl_model = Sequential()\n",
    "        dl_model.add(Dense(64, activation='relu', input_shape=(X_copy.shape[1],)))\n",
    "        dl_model.add(Dropout(0.2))\n",
    "        dl_model.add(Dense(32, activation='relu'))\n",
    "        dl_model.add(Dropout(0.2))\n",
    "        if TASK_TYPE == \"classification\":\n",
    "            dl_model.add(Dense(1, activation='sigmoid'))\n",
    "            dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        else:\n",
    "            dl_model.add(Dense(1))\n",
    "            dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # Train Deep Learning model\n",
    "        dl_model.fit(X_copy, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
    "        models['Deep Learning'] = dl_model\n",
    "        # print(\"Deep Learning model trained successfully!\")\n",
    "\n",
    "        # X_train_seq, y_train_seq = create_sequences(X_train, y_train)\n",
    "        # # LSTM Model\n",
    "        # time_steps = 60\n",
    "        # lstm_model = Sequential()\n",
    "        # lstm_model.add(LSTM(50, activation='relu', input_shape=(time_steps, X_train_seq.shape[1]), return_sequences=False))\n",
    "        # lstm_model.add(Dropout(0.2))\n",
    "        # lstm_model.add(Dense(25, activation='relu'))\n",
    "        # lstm_model.add(Dropout(0.2))\n",
    "        # if TASK_TYPE == \"classification\":\n",
    "        #     lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "        #     lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        # else:\n",
    "        #     lstm_model.add(Dense(1))\n",
    "        #     lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        \n",
    "        # # Train LSTM model\n",
    "        # lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
    "        # models['LSTM'] = lstm_model\n",
    "        # print(\"LSTM model trained successfully!\")\n",
    "        \n",
    "        # Run AutoML\n",
    "        #automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)\n",
    "        #models['AutoML'] = automl_model\n",
    "\n",
    "        return models, X_train, X_test, y_train, y_test\n",
    "\n",
    "    # Train models\n",
    "    models, X_train, X_test, y_train, y_test = train_models(X, y)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------\n",
    "    # model eval\n",
    "    def model_weights(mod):\n",
    "        # 3. Access and print the weights\n",
    "        weights = mod.coef_\n",
    "        intercept = mod.intercept_\n",
    "\n",
    "        print(f\"Weights (Coefficients): {weights}\")\n",
    "        print(f\"Intercept: {intercept}\")\n",
    "\n",
    "        # 4. Optional: Create a DataFrame for better readability\n",
    "        # This pairs each weight with its corresponding feature name\n",
    "        weights_df = pd.DataFrame({'Feature': X.columns, 'Weight': weights})\n",
    "        print(\"\\nModel Weights:\")\n",
    "        print(weights_df)\n",
    "\n",
    "    def evaluate_models(models, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate trained models using appropriate metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        models (dict): Dictionary of trained models\n",
    "        X_test (pandas.DataFrame): Test features\n",
    "        y_test (numpy.array): Test target\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            if name == 'Deep Learning':\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred = (y_pred > 0.5).astype(int).flatten() if TASK_TYPE == \"classification\" else y_pred.flatten()\n",
    "            else:\n",
    "                y_pred = model.predict(X_test)\n",
    "            \n",
    "            if TASK_TYPE == \"classification\":\n",
    "                results[name] = {\n",
    "                    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                    'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "                    'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "                    'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
    "                }\n",
    "            else:\n",
    "                results[name] = {\n",
    "                    'Ticker': TICKER_LIST[filec-1],\n",
    "                    'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                    'R2 Score': r2_score(y_test, y_pred)\n",
    "                }\n",
    "\n",
    "        \n",
    "        # Display results\n",
    "        results_df = pd.DataFrame(results).T\n",
    "        # print(\"\\nModel Evaluation Results:\")\n",
    "        # print(results_df)\n",
    "        \n",
    "        return results_df\n",
    "\n",
    "    # Evaluate models\n",
    "    results_df = evaluate_models(models, X_test, y_test)\n",
    "    print(results_df)\n",
    "    \n",
    "    # results_df[TICKER_LIST[filec]] = evaluate_models(models, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
