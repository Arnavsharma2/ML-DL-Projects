{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Data Science/ML Project Template\n",
    "\n",
    "**Author**: Arnav Sharma arnav.sharma2264@gmail.com\n",
    "**Date**: 2025-08-29  \n",
    "**Description**: A comprehensive template for building, evaluating, and deploying machine learning and deep learning models.\n",
    "\n",
    "This notebook provides a structured framework for an end-to-end Data Science/ML project, covering data loading, preprocessing, exploratory data analysis (EDA), feature engineering, model training, AutoML, deep learning, evaluation, and deployment. The template is modular and adaptable for various datasets and ML tasks (classification, regression, etc.).\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Setup](#1-project-setup)\n",
    "2. [Data Loading](#2-data-loading)\n",
    "3. [Exploratory Data Analysis (EDA)](#3-exploratory-data-analysis-eda)\n",
    "4. [Data Preprocessing](#4-data-preprocessing)\n",
    "5. [Feature Engineering](#5-feature-engineering)\n",
    "6. [Model Training](#6-model-training)\n",
    "7. [AutoML](#7-automl)\n",
    "8. [Deep Learning](#8-deep-learning)\n",
    "9. [Model Evaluation](#9-model-evaluation)\n",
    "10. [Hyperparameter Tuning](#10-hyperparameter-tuning)\n",
    "11. [Model Interpretation](#11-model-interpretation)\n",
    "12. [Model Deployment](#12-model-deployment)\n",
    "13. [Conclusion and Next Steps](#13-conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "Install and import necessary libraries, set random seeds for reproducibility, and configure project settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aps/Desktop/ML-DL-Projects/env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment to install)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from flaml import AutoML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import shap\n",
    "from skopt import BayesSearchCV\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define project-specific variables\n",
    "DATA_PATH = \"path/to/your/dataset.csv\"  # Update with your dataset path\n",
    "TARGET_COLUMN = \"target\"  # Update with your target column name\n",
    "TASK_TYPE = \"classification\"  # Options: \"classification\" or \"regression\"\n",
    "MODEL_SAVE_PATH = \"model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "Load the dataset and perform initial checks for data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load dataset from a given file path.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset (CSV, Excel, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            data = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "        \n",
    "        print(\"Data loaded successfully!\")\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(data.head())\n",
    "        print(\"\\nData Info:\")\n",
    "        print(data.info())\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data(DATA_PATH)\n",
    "if df is None:\n",
    "    raise SystemExit(\"Data loading failed. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "Understand the dataset through visualizations and statistical summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df, target_column):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataset\n",
    "    target_column (str): Name of the target column\n",
    "    \"\"\"\n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    # Distribution of target variable\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if TASK_TYPE == \"classification\":\n",
    "        sns.countplot(x=target_column, data=df)\n",
    "        plt.title(f\"Distribution of {target_column}\")\n",
    "    else:\n",
    "        sns.histplot(df[target_column], kde=True)\n",
    "        plt.title(f\"Distribution of {target_column} (Regression)\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation matrix (for numeric columns)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Pairplot for numeric features (optional, for smaller datasets)\n",
    "    # sns.pairplot(df[numeric_cols])\n",
    "    # plt.show()\n",
    "\n",
    "# Perform EDA\n",
    "perform_eda(df, TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "Clean the data and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset (handle missing values, encode categorical variables, etc.).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataset\n",
    "    target_column (str): Name of the target column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Preprocessed dataset\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Impute numeric columns with median\n",
    "    for col in numeric_cols:\n",
    "        X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Impute categorical columns with mode\n",
    "    for col in categorical_cols:\n",
    "        X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "    \n",
    "    # Encode target variable (for classification)\n",
    "    if TASK_TYPE == \"classification\":\n",
    "        y = le.fit_transform(y)\n",
    "    \n",
    "    print(\"Data preprocessing completed!\")\n",
    "    return X, y\n",
    "\n",
    "# Preprocess data\n",
    "X, y = preprocess_data(df, TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "Create new features and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X):\n",
    "    \"\"\"\n",
    "    Perform feature engineering (e.g., creating new features, scaling).\n",
    "    \n",
    "    Parameters:\n",
    "    X (pandas.DataFrame): Feature dataset\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Engineered feature dataset\n",
    "    \"\"\"\n",
    "    # Example: Create interaction features (customize as needed)\n",
    "    if 'feature1' in X.columns and 'feature2' in X.columns:\n",
    "        X['feature1_feature2_interaction'] = X['feature1'] * X['feature2']\n",
    "    \n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "    \n",
    "    print(\"Feature engineering completed!\")\n",
    "    return X, scaler\n",
    "\n",
    "# Perform feature engineering\n",
    "X, scaler = feature_engineering(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "Split data and train multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X, y):\n",
    "    \"\"\"\n",
    "    Train multiple machine learning models, AutoML, and a deep learning model.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pandas.DataFrame): Features\n",
    "    y (numpy.array): Target\n",
    "    \n",
    "    Returns:\n",
    "    dict: Trained models (including AutoML and Deep Learning)\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize models based on task type\n",
    "    models = {}\n",
    "    if TASK_TYPE == \"classification\":\n",
    "        models['Logistic Regression'] = LogisticRegression(random_state=42)\n",
    "        models['Random Forest'] = RandomForestClassifier(random_state=42)\n",
    "        models['XGBoost'] = XGBClassifier(random_state=42)\n",
    "    else:\n",
    "        models['Linear Regression'] = LinearRegression()\n",
    "        models['Random Forest'] = RandomForestRegressor(random_state=42)\n",
    "        models['XGBoost'] = XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Train manual models\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"{name} trained successfully!\")\n",
    "    \n",
    "    # Run AutoML\n",
    "    automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)\n",
    "    models['AutoML'] = automl_model\n",
    "    \n",
    "    # Train deep learning model\n",
    "    dl_model = train_deep_learning_model(X_train, y_train, TASK_TYPE)\n",
    "    models['Deep Learning'] = dl_model\n",
    "    \n",
    "    return models, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Train models\n",
    "models, X_train, X_test, y_train, y_test = train_models(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AutoML\n",
    "Use an AutoML framework (FLAML) to automatically select and tune the best model for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "def run_automl(X_train, y_train, task_type, time_budget=60):\n",
    "    \"\"\"\n",
    "    Run AutoML using FLAML to automatically select and tune the best model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    y_train (numpy.array): Training target\n",
    "    task_type (str): 'classification' or 'regression'\n",
    "    time_budget (int): Time budget in seconds for AutoML (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "    model: Best AutoML model\n",
    "    \"\"\"\n",
    "    automl = AutoML()\n",
    "    automl_settings = {\n",
    "        \"time_budget\": time_budget,  # Time budget in seconds\n",
    "        \"metric\": \"accuracy\" if task_type == \"classification\" else \"rmse\",\n",
    "        \"task\": task_type,\n",
    "        \"log_file_name\": \"automl.log\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
    "    print(f\"Best AutoML model: {automl.best_estimator}\")\n",
    "    print(f\"Best configuration: {automl.best_config}\")\n",
    "    \n",
    "    return automl\n",
    "\n",
    "# Run AutoML\n",
    "automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Learning\n",
    "Train a deep learning model using TensorFlow/Keras for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_deep_learning_model(input_dim, task_type):\n",
    "    \"\"\"\n",
    "    Build a simple feedforward neural network for classification or regression.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dim (int): Number of input features\n",
    "    task_type (str): 'classification' or 'regression'\n",
    "    \n",
    "    Returns:\n",
    "    keras.Model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    if task_type == \"classification\":\n",
    "        model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "        loss = 'binary_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "    else:\n",
    "        model.add(Dense(1))  # Regression\n",
    "        loss = 'mse'\n",
    "        metrics = ['mse']\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "def train_deep_learning_model(X_train, y_train, task_type):\n",
    "    \"\"\"\n",
    "    Train the deep learning model with early stopping.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    y_train (numpy.array): Training target\n",
    "    task_type (str): 'classification' or 'regression'\n",
    "    \n",
    "    Returns:\n",
    "    keras.Model: Trained model\n",
    "    \"\"\"\n",
    "    model = build_deep_learning_model(X_train.shape[1], task_type)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.2,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train deep learning model\n",
    "dl_model = train_deep_learning_model(X_train, y_train, TASK_TYPE)\n",
    "models['Deep Learning'] = dl_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "Evaluate models using appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate trained models using appropriate metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    models (dict): Dictionary of trained models\n",
    "    X_test (pandas.DataFrame): Test features\n",
    "    y_test (numpy.array): Test target\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'Deep Learning':\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = (y_pred > 0.5).astype(int).flatten() if TASK_TYPE == \"classification\" else y_pred.flatten()\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if TASK_TYPE == \"classification\":\n",
    "            results[name] = {\n",
    "                'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "                'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "                'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
    "            }\n",
    "        else:\n",
    "            results[name] = {\n",
    "                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'R2 Score': r2_score(y_test, y_pred)\n",
    "            }\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Evaluate models\n",
    "results_df = evaluate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "Optimize the best-performing model using Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_best_model(X_train, y_train, best_model_name, models):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the best model (excluding AutoML and Deep Learning).\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    y_train (numpy.array): Training target\n",
    "    best_model_name (str): Name of the best model\n",
    "    models (dict): Dictionary of trained models\n",
    "    \n",
    "    Returns:\n",
    "    model: Tuned model\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space (customize as needed)\n",
    "    param_space = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': (10, 200),\n",
    "            'max_depth': (3, 20),\n",
    "            'min_samples_split': (2, 10)\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': (10, 200),\n",
    "            'max_depth': (3, 20),\n",
    "            'learning_rate': (0.01, 0.3, 'log-uniform')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if best_model_name in param_space:\n",
    "        bayes_cv = BayesSearchCV(\n",
    "            estimator=models[best_model_name],\n",
    "            search_spaces=param_space[best_model_name],\n",
    "            n_iter=20,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        bayes_cv.fit(X_train, y_train)\n",
    "        print(f\"Best parameters for {best_model_name}: {bayes_cv.best_params_}\")\n",
    "        return bayes_cv.best_estimator_\n",
    "    elif best_model_name in ['AutoML', 'Deep Learning']:\n",
    "        print(f\"{best_model_name} model is already tuned or does not support BayesSearchCV. Skipping additional tuning.\")\n",
    "        return models[best_model_name]\n",
    "    else:\n",
    "        print(f\"No tuning defined for {best_model_name}\")\n",
    "        return models[best_model_name]\n",
    "\n",
    "# Select the best model (based on evaluation metrics)\n",
    "best_model_name = results_df.idxmax()['Accuracy' if TASK_TYPE == \"classification\" else 'R2 Score']\n",
    "tuned_model = tune_best_model(X_train, y_train, best_model_name, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Interpretation\n",
    "Interpret the model using SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_model(model, X_train, model_name):\n",
    "    \"\"\"\n",
    "    Interpret the model using SHAP values.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_name in ['Random Forest', 'XGBoost']:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_train)\n",
    "        elif model_name == 'AutoML':\n",
    "            # Check if the AutoML model is tree-based\n",
    "            if 'rf' in model.best_estimator.lower() or 'xgboost' in model.best_estimator.lower():\n",
    "                explainer = shap.TreeExplainer(model.model)\n",
    "                shap_values = explainer.shap_values(X_train)\n",
    "            else:\n",
    "                explainer = shap.KernelExplainer(model.predict, X_train)\n",
    "                shap_values = explainer.shap_values(X_train)\n",
    "        elif model_name == 'Deep Learning':\n",
    "            explainer = shap.DeepExplainer(model, X_train)\n",
    "            shap_values = explainer.shap_values(X_train.to_numpy())\n",
    "        else:\n",
    "            explainer = shap.KernelExplainer(model.predict, X_train)\n",
    "            shap_values = explainer.shap_values(X_train)\n",
    "        \n",
    "        # Summary plot\n",
    "        shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "        plt.title(f\"Feature Importance (SHAP) - {model_name}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP interpretation failed for {model_name}: {e}\")\n",
    "\n",
    "# Interpret the best model\n",
    "interpret_model(tuned_model, X_train, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Deployment\n",
    "Save the model and create a prediction function for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, path, model_name):\n",
    "    \"\"\"\n",
    "    Save the trained model and scaler.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    scaler: Fitted scaler\n",
    "    path (str): Path to save the model\n",
    "    model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    if model_name == 'Deep Learning':\n",
    "        model.save(path.replace('.pkl', '.h5'))  # Save Keras model in HDF5 format\n",
    "        joblib.dump({'scaler': scaler}, path.replace('.pkl', '_scaler.pkl'))\n",
    "        print(f\"Deep Learning model saved to {path.replace('.pkl', '.h5')}\")\n",
    "        print(f\"Scaler saved to {path.replace('.pkl', '_scaler.pkl')}\")\n",
    "    else:\n",
    "        joblib.dump({'model': model, 'scaler': scaler}, path)\n",
    "        print(f\"Model and scaler saved to {path}\")\n",
    "\n",
    "def predict_new_data(model, scaler, new_data, model_name):\n",
    "    \"\"\"\n",
    "    Make predictions on new data.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    scaler: Fitted scaler\n",
    "    new_data (pandas.DataFrame): New data for prediction\n",
    "    model_name (str): Name of the model\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: Predictions\n",
    "    \"\"\"\n",
    "    # Preprocess new data\n",
    "    numeric_cols = new_data.select_dtypes(include=[np.number]).columns\n",
    "    new_data[numeric_cols] = scaler.transform(new_data[numeric_cols])\n",
    "    \n",
    "    # Make predictions\n",
    "    if model_name == 'Deep Learning':\n",
    "        predictions = model.predict(new_data).flatten()\n",
    "        if TASK_TYPE == \"classification\":\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "    else:\n",
    "        predictions = model.predict(new_data)\n",
    "    return predictions\n",
    "\n",
    "# Save the model\n",
    "save_model(tuned_model, scaler, MODEL_SAVE_PATH, best_model_name)\n",
    "\n",
    "# Example: Predict on new data (replace with actual new data)\n",
    "# new_data = pd.DataFrame(...)  # Load or create new data\n",
    "# predictions = predict_new_data(tuned_model, scaler, new_data, best_model_name)\n",
    "# print(\"Predictions on new data:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Next Steps\n",
    "- **Summary**: Summarize the findings, best model performance (including AutoML and Deep Learning results), and key insights from EDA and model interpretation.\n",
    "- **Next Steps**: Consider additional feature engineering, trying other AutoML frameworks, experimenting with different neural network architectures, or deploying the model in a production environment.\n",
    "- **Monitoring**: Plan for model monitoring and retraining to handle data drift.\n",
    "\n",
    "```python\n",
    "print(\"End-to-End ML and Deep Learning Project Completed!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End-to-End ML and Deep Learning Project Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
